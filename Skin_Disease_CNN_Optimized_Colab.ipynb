{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skin_disease_title"
      },
      "source": [
        "# 🩺 SwasthVedha Skin Disease Classification - OPTIMIZED ResNet50\n",
        "## GPU-Accelerated Training with Hair Disease Model Techniques\n",
        "\n",
        "**Target**: Improve from 44.83% to 90%+ accuracy\n",
        "**Model**: ResNet50 with advanced transfer learning\n",
        "**Strategy**: Apply same techniques that achieved 100% on Hair Disease\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpu_setup"
      },
      "source": [
        "## 🔧 GPU Setup & Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup_gpu"
      },
      "outputs": [],
      "source": [
        "# GPU Check and imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "from PIL import Image\n",
        "import json\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name()}\")\n",
        "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "data_upload_section"
      },
      "source": [
        "## 📂 Upload Your Skin Disease Dataset\n",
        "\n",
        "**Instructions:**\n",
        "1. Zip your skin disease dataset folder\n",
        "2. Make sure it has `train/`, `val/`, `test/` folders\n",
        "3. Upload the zip file below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upload_skin_dataset"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import zipfile\n",
        "\n",
        "print(\"Please upload your Skin Disease dataset (zipped):\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Extract the uploaded zip file\n",
        "for filename in uploaded.keys():\n",
        "    print(f\"Extracting {filename}...\")\n",
        "    with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
        "        zip_ref.extractall('/content/')\n",
        "    print(f\"Extracted {filename}\")\n",
        "\n",
        "# Find the dataset directory\n",
        "dataset_paths = []\n",
        "for root, dirs, files in os.walk('/content/'):\n",
        "    if ('train' in dirs or 'train_set' in dirs) and ('test' in dirs or 'test_set' in dirs):\n",
        "        dataset_paths.append(root)\n",
        "\n",
        "if dataset_paths:\n",
        "    data_dir = dataset_paths[0]\n",
        "    print(f\"Dataset found at: {data_dir}\")\n",
        "    \n",
        "    # Check for different folder naming conventions\n",
        "    if os.path.exists(os.path.join(data_dir, 'train_set')):\n",
        "        train_dir = os.path.join(data_dir, 'train_set')\n",
        "        test_dir = os.path.join(data_dir, 'test_set')\n",
        "        val_dir = os.path.join(data_dir, 'val_set') if os.path.exists(os.path.join(data_dir, 'val_set')) else None\n",
        "    else:\n",
        "        train_dir = os.path.join(data_dir, 'train')\n",
        "        test_dir = os.path.join(data_dir, 'test')\n",
        "        val_dir = os.path.join(data_dir, 'val') if os.path.exists(os.path.join(data_dir, 'val')) else None\n",
        "    \n",
        "    print(f\"Train dir: {train_dir}\")\n",
        "    print(f\"Test dir: {test_dir}\")\n",
        "    print(f\"Val dir: {val_dir}\")\n",
        "    \n",
        "else:\n",
        "    print(\"Dataset structure not found. Please ensure your zip contains train/test folders.\")\n",
        "    data_dir = '/content/skin_disease'  # fallback"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "advanced_transforms"
      },
      "source": [
        "## 🔄 Advanced Data Preprocessing (Hair Disease Model Style)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_transforms"
      },
      "outputs": [],
      "source": [
        "# Advanced transforms - same as Hair Disease model that got 100%\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomVerticalFlip(p=0.3),\n",
        "        transforms.RandomRotation(30),\n",
        "        transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n",
        "        transforms.RandomAffine(degrees=0, translate=(0.15, 0.15), scale=(0.85, 1.15)),\n",
        "        transforms.RandomPerspective(distortion_scale=0.2, p=0.3),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "        transforms.RandomErasing(p=0.2, scale=(0.02, 0.15))\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'test': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "}\n",
        "\n",
        "# Create datasets\n",
        "if val_dir and os.path.exists(val_dir):\n",
        "    # Use separate validation set\n",
        "    image_datasets = {\n",
        "        'train': datasets.ImageFolder(train_dir, data_transforms['train']),\n",
        "        'val': datasets.ImageFolder(val_dir, data_transforms['val']),\n",
        "        'test': datasets.ImageFolder(test_dir, data_transforms['test'])\n",
        "    }\n",
        "else:\n",
        "    # Split training set for validation\n",
        "    full_train_dataset = datasets.ImageFolder(train_dir, data_transforms['train'])\n",
        "    train_size = int(0.8 * len(full_train_dataset))\n",
        "    val_size = len(full_train_dataset) - train_size\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(full_train_dataset, [train_size, val_size])\n",
        "    \n",
        "    # Apply validation transforms to validation split\n",
        "    val_dataset.dataset.transform = data_transforms['val']\n",
        "    \n",
        "    image_datasets = {\n",
        "        'train': train_dataset,\n",
        "        'val': val_dataset,\n",
        "        'test': datasets.ImageFolder(test_dir, data_transforms['test'])\n",
        "    }\n",
        "\n",
        "# Create data loaders with optimized settings\n",
        "dataloaders = {\n",
        "    'train': torch.utils.data.DataLoader(image_datasets['train'], batch_size=24, shuffle=True, num_workers=4, pin_memory=True),\n",
        "    'val': torch.utils.data.DataLoader(image_datasets['val'], batch_size=32, shuffle=False, num_workers=4, pin_memory=True),\n",
        "    'test': torch.utils.data.DataLoader(image_datasets['test'], batch_size=32, shuffle=False, num_workers=4, pin_memory=True)\n",
        "}\n",
        "\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val', 'test']}\n",
        "\n",
        "# Get class names\n",
        "if hasattr(image_datasets['train'], 'classes'):\n",
        "    class_names = image_datasets['train'].classes\n",
        "else:\n",
        "    class_names = image_datasets['train'].dataset.classes\n",
        "\n",
        "print(f\"Dataset sizes:\")\n",
        "for phase in ['train', 'val', 'test']:\n",
        "    print(f\"  {phase}: {dataset_sizes[phase]} images\")\n",
        "    \n",
        "print(f\"\\nClasses ({len(class_names)}): {class_names}\")\n",
        "print(f\"Device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "optimized_model"
      },
      "source": [
        "## 🤖 Optimized ResNet50 Model (Hair Disease Architecture)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_optimized_model"
      },
      "outputs": [],
      "source": [
        "def create_optimized_skin_model(num_classes):\n",
        "    \"\"\"Create optimized ResNet50 model - same as Hair Disease model\"\"\"\n",
        "    # Load pre-trained ResNet50\n",
        "    model = models.resnet50(pretrained=True)\n",
        "    \n",
        "    # Freeze early layers (same as Hair model)\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "    \n",
        "    # Unfreeze last layers for fine-tuning\n",
        "    for param in model.layer4.parameters():\n",
        "        param.requires_grad = True\n",
        "    for param in model.layer3[-1].parameters():\n",
        "        param.requires_grad = True\n",
        "    \n",
        "    # Advanced classifier (improved from Hair model)\n",
        "    num_ftrs = model.fc.in_features\n",
        "    model.fc = nn.Sequential(\n",
        "        nn.Dropout(0.6),\n",
        "        nn.Linear(num_ftrs, 1024),\n",
        "        nn.BatchNorm1d(1024),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.4),\n",
        "        nn.Linear(1024, 512),\n",
        "        nn.BatchNorm1d(512),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.3),\n",
        "        nn.Linear(512, num_classes)\n",
        "    )\n",
        "    \n",
        "    return model\n",
        "\n",
        "# Create model\n",
        "model = create_optimized_skin_model(len(class_names))\n",
        "model = model.to(device)\n",
        "\n",
        "# Advanced loss function with label smoothing\n",
        "class LabelSmoothingLoss(nn.Module):\n",
        "    def __init__(self, classes, smoothing=0.1):\n",
        "        super(LabelSmoothingLoss, self).__init__()\n",
        "        self.confidence = 1.0 - smoothing\n",
        "        self.smoothing = smoothing\n",
        "        self.cls = classes\n",
        "        self.dim = -1\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        pred = pred.log_softmax(dim=self.dim)\n",
        "        with torch.no_grad():\n",
        "            true_dist = torch.zeros_like(pred)\n",
        "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
        "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
        "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))\n",
        "\n",
        "criterion = LabelSmoothingLoss(len(class_names), smoothing=0.1)\n",
        "\n",
        "# Advanced optimizer with different learning rates\n",
        "optimizer = optim.AdamW([\n",
        "    {'params': model.layer3[-1].parameters(), 'lr': 5e-5},\n",
        "    {'params': model.layer4.parameters(), 'lr': 1e-4},\n",
        "    {'params': model.fc.parameters(), 'lr': 1e-3}\n",
        "], weight_decay=1e-4)\n",
        "\n",
        "# Advanced scheduler\n",
        "scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=[5e-5, 1e-4, 1e-3], \n",
        "                                   steps_per_epoch=len(dataloaders['train']), \n",
        "                                   epochs=40, pct_start=0.3)\n",
        "\n",
        "print(f\"Model created with {len(class_names)} classes\")\n",
        "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "training_function"
      },
      "source": [
        "## 🚀 Advanced Training Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "advanced_training"
      },
      "outputs": [],
      "source": [
        "def train_optimized_model(model, criterion, optimizer, scheduler, num_epochs=40):\n",
        "    \"\"\"Advanced training with all optimizations\"\"\"\n",
        "    since = time.time()\n",
        "    \n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "    patience = 8\n",
        "    patience_counter = 0\n",
        "    \n",
        "    # Training history\n",
        "    history = {\n",
        "        'train_loss': [], 'train_acc': [],\n",
        "        'val_loss': [], 'val_acc': []\n",
        "    }\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "        print('-' * 50)\n",
        "        \n",
        "        # Each epoch has training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "                \n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "            \n",
        "            # Progress tracking\n",
        "            total_batches = len(dataloaders[phase])\n",
        "            \n",
        "            for batch_idx, (inputs, labels) in enumerate(dataloaders[phase]):\n",
        "                inputs = inputs.to(device, non_blocking=True)\n",
        "                labels = labels.to(device, non_blocking=True)\n",
        "                \n",
        "                optimizer.zero_grad()\n",
        "                \n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    # Mixed precision for speed\n",
        "                    with torch.cuda.amp.autocast():\n",
        "                        outputs = model(inputs)\n",
        "                        loss = criterion(outputs, labels)\n",
        "                        _, preds = torch.max(outputs, 1)\n",
        "                    \n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        # Gradient clipping\n",
        "                        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "                        optimizer.step()\n",
        "                        scheduler.step()\n",
        "                        \n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "                \n",
        "                # Progress update every 20 batches\n",
        "                if batch_idx % 20 == 0:\n",
        "                    print(f'  {phase.capitalize()} batch {batch_idx+1}/{total_batches}', end='\\r')\n",
        "                \n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "            \n",
        "            print(f'{phase.capitalize()} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "            \n",
        "            # Save history\n",
        "            history[f'{phase}_loss'].append(epoch_loss)\n",
        "            history[f'{phase}_acc'].append(epoch_acc.item())\n",
        "            \n",
        "            # Save best model and early stopping\n",
        "            if phase == 'val':\n",
        "                if epoch_acc > best_acc:\n",
        "                    best_acc = epoch_acc\n",
        "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                    patience_counter = 0\n",
        "                    \n",
        "                    # Save checkpoint\n",
        "                    torch.save({\n",
        "                        'epoch': epoch + 1,\n",
        "                        'model_state_dict': model.state_dict(),\n",
        "                        'optimizer_state_dict': optimizer.state_dict(),\n",
        "                        'best_acc': best_acc,\n",
        "                        'class_names': class_names\n",
        "                    }, f'/content/skin_resnet50_best.pth')\n",
        "                    \n",
        "                    print(f'*** New best validation accuracy: {best_acc:.4f} ***')\n",
        "                else:\n",
        "                    patience_counter += 1\n",
        "                    \n",
        "                # Early stopping\n",
        "                if patience_counter >= patience:\n",
        "                    print(f'Early stopping triggered after {patience} epochs without improvement')\n",
        "                    break\n",
        "        \n",
        "        print(f'Current best val Acc: {best_acc:.4f}\\n')\n",
        "        \n",
        "        # Break from outer loop too if early stopping\n",
        "        if patience_counter >= patience:\n",
        "            break\n",
        "        \n",
        "    time_elapsed = time.time() - since\n",
        "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "    print(f'Best val Acc: {best_acc:.4f}')\n",
        "    \n",
        "    # Load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, history, best_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "start_training"
      },
      "source": [
        "## 🔥 Start Optimized Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_training"
      },
      "outputs": [],
      "source": [
        "# Start training\n",
        "print(\"🚀 Starting OPTIMIZED Skin Disease Classification Training...\")\n",
        "print(f\"Training on {dataset_sizes['train']} images\")\n",
        "print(f\"Validating on {dataset_sizes['val']} images\")\n",
        "print(f\"Testing on {dataset_sizes['test']} images\")\n",
        "print(f\"Classes: {len(class_names)}\")\n",
        "print(f\"Device: {device}\")\n",
        "print(f\"Target: Improve from 44.83% to 90%+\\n\")\n",
        "\n",
        "# Train the model\n",
        "model, history, best_val_acc = train_optimized_model(model, criterion, optimizer, scheduler, num_epochs=40)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "test_model"
      },
      "source": [
        "## 📊 Comprehensive Model Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "comprehensive_testing"
      },
      "outputs": [],
      "source": [
        "# Comprehensive testing function\n",
        "def comprehensive_test(model):\n",
        "    \"\"\"Comprehensive testing with detailed metrics\"\"\"\n",
        "    model.eval()\n",
        "    running_corrects = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    class_correct = list(0. for i in range(len(class_names)))\n",
        "    class_total = list(0. for i in range(len(class_names)))\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloaders['test']:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "            \n",
        "            # Store for detailed analysis\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            \n",
        "            # Per-class accuracy\n",
        "            c = (preds == labels).squeeze()\n",
        "            for i in range(labels.size(0)):\n",
        "                label = labels[i]\n",
        "                class_correct[label] += c[i].item()\n",
        "                class_total[label] += 1\n",
        "    \n",
        "    test_acc = running_corrects.double() / dataset_sizes['test']\n",
        "    print(f'🎯 FINAL TEST ACCURACY: {test_acc:.4f} ({test_acc*100:.2f}%)')\n",
        "    \n",
        "    # Per-class accuracy\n",
        "    print('\\n📊 Per-class accuracy:')\n",
        "    for i in range(len(class_names)):\n",
        "        if class_total[i] > 0:\n",
        "            acc = 100 * class_correct[i] / class_total[i]\n",
        "            print(f'{class_names[i]}: {acc:.1f}% ({int(class_correct[i])}/{int(class_total[i])})')\n",
        "    \n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "                xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.title('Confusion Matrix - Skin Disease Classification')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.yticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Classification report\n",
        "    print('\\n📋 Detailed Classification Report:')\n",
        "    print(classification_report(all_labels, all_preds, target_names=class_names))\n",
        "    \n",
        "    return test_acc, all_preds, all_labels\n",
        "\n",
        "# Run comprehensive testing\n",
        "test_accuracy, predictions, true_labels = comprehensive_test(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "visualize_results"
      },
      "source": [
        "## 📈 Training Results Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plot_results"
      },
      "outputs": [],
      "source": [
        "# Plot comprehensive training history\n",
        "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "# Training & Validation Accuracy\n",
        "ax1.plot(history['train_acc'], label='Training Accuracy', linewidth=2)\n",
        "ax1.plot(history['val_acc'], label='Validation Accuracy', linewidth=2)\n",
        "ax1.set_title('Model Accuracy Over Time', fontsize=14, fontweight='bold')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Accuracy')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.set_ylim([0, 1.1])\n",
        "\n",
        "# Training & Validation Loss\n",
        "ax2.plot(history['train_loss'], label='Training Loss', linewidth=2, color='red')\n",
        "ax2.plot(history['val_loss'], label='Validation Loss', linewidth=2, color='orange')\n",
        "ax2.set_title('Model Loss Over Time', fontsize=14, fontweight='bold')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Loss')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# Accuracy improvement comparison\n",
        "old_acc = 44.83\n",
        "new_acc = float(test_accuracy * 100)\n",
        "improvement = new_acc - old_acc\n",
        "\n",
        "ax3.bar(['Previous Model\\n(ResNet50)', 'Optimized Model\\n(This Training)'], \n",
        "        [old_acc, new_acc], \n",
        "        color=['lightcoral', 'lightgreen'], \n",
        "        edgecolor=['red', 'darkgreen'], linewidth=2)\n",
        "ax3.set_title(f'Accuracy Improvement: +{improvement:.1f}%', fontsize=14, fontweight='bold')\n",
        "ax3.set_ylabel('Test Accuracy (%)')\n",
        "ax3.set_ylim([0, 100])\n",
        "for i, v in enumerate([old_acc, new_acc]):\n",
        "    ax3.text(i, v + 2, f'{v:.1f}%', ha='center', fontweight='bold', fontsize=12)\n",
        "\n",
        "# Performance grade\n",
        "if new_acc >= 90:\n",
        "    grade = 'A+ (Exceptional)'\n",
        "    color = 'darkgreen'\n",
        "elif new_acc >= 80:\n",
        "    grade = 'A (Excellent)'\n",
        "    color = 'green'\n",
        "elif new_acc >= 70:\n",
        "    grade = 'B (Good)'\n",
        "    color = 'orange'\n",
        "elif new_acc >= 60:\n",
        "    grade = 'C (Fair)'\n",
        "    color = 'yellow'\n",
        "else:\n",
        "    grade = 'D (Needs Improvement)'\n",
        "    color = 'red'\n",
        "\n",
        "ax4.text(0.5, 0.6, f'FINAL GRADE', ha='center', fontsize=16, fontweight='bold')\n",
        "ax4.text(0.5, 0.4, grade, ha='center', fontsize=20, fontweight='bold', color=color)\n",
        "ax4.text(0.5, 0.2, f'Test Accuracy: {new_acc:.2f}%', ha='center', fontsize=14)\n",
        "ax4.set_xlim([0, 1])\n",
        "ax4.set_ylim([0, 1])\n",
        "ax4.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print comprehensive results\n",
        "print(f\"\\n🎯 COMPREHENSIVE RESULTS SUMMARY:\")\n",
        "print(f\"=\" * 50)\n",
        "print(f\"📊 Previous Model Accuracy:     {old_acc:.2f}%\")\n",
        "print(f\"🚀 New Optimized Accuracy:     {new_acc:.2f}%\")\n",
        "print(f\"📈 Improvement:                +{improvement:.2f}%\")\n",
        "print(f\"🏆 Performance Grade:          {grade}\")\n",
        "print(f\"✅ Best Validation Accuracy:   {float(best_val_acc)*100:.2f}%\")\n",
        "print(f\"\\n🎉 Model Status: {'PRODUCTION READY!' if new_acc >= 80 else 'Needs More Training'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "save_download_model"
      },
      "source": [
        "## 💾 Save & Download Optimized Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "save_final_model"
      },
      "outputs": [],
      "source": [
        "# Save comprehensive model with all metadata\n",
        "final_model_path = '/content/skin_disease_resnet50_optimized.pth'\n",
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'class_names': class_names,\n",
        "    'test_accuracy': float(test_accuracy),\n",
        "    'val_accuracy': float(best_val_acc),\n",
        "    'improvement': float(test_accuracy * 100 - 44.83),\n",
        "    'history': history,\n",
        "    'model_architecture': 'Optimized ResNet50',\n",
        "    'training_details': {\n",
        "        'optimizer': 'AdamW with different LRs',\n",
        "        'scheduler': 'OneCycleLR',\n",
        "        'augmentations': 'Advanced (10+ transforms)',\n",
        "        'label_smoothing': 0.1,\n",
        "        'mixed_precision': True,\n",
        "        'gradient_clipping': True\n",
        "    }\n",
        "}, final_model_path)\n",
        "\n",
        "# Save class mapping\n",
        "class_mapping = {str(i): class_name for i, class_name in enumerate(class_names)}\n",
        "with open('/content/skin_class_mapping.json', 'w') as f:\n",
        "    json.dump(class_mapping, f, indent=2)\n",
        "\n",
        "# Save comprehensive model info\n",
        "new_acc = float(test_accuracy * 100)\n",
        "improvement = new_acc - 44.83\n",
        "\n",
        "model_info = {\n",
        "    \"model_name\": \"Skin Disease Classification ResNet50 OPTIMIZED\",\n",
        "    \"architecture\": \"ResNet50 with advanced classifier\",\n",
        "    \"num_classes\": len(class_names),\n",
        "    \"classes\": class_names,\n",
        "    \"performance\": {\n",
        "        \"test_accuracy\": f\"{new_acc:.4f}\",\n",
        "        \"val_accuracy\": f\"{float(best_val_acc)*100:.4f}\",\n",
        "        \"previous_accuracy\": \"44.83\",\n",
        "        \"improvement\": f\"{improvement:.2f}%\",\n",
        "        \"grade\": grade\n",
        "    },\n",
        "    \"technical_specs\": {\n",
        "        \"input_size\": [224, 224],\n",
        "        \"preprocessing\": \"ImageNet normalization + advanced augmentation\",\n",
        "        \"optimizer\": \"AdamW with layer-specific learning rates\",\n",
        "        \"scheduler\": \"OneCycleLR\",\n",
        "        \"regularization\": \"Dropout + BatchNorm + Weight Decay + Label Smoothing\"\n",
        "    },\n",
        "    \"training_environment\": \"Google Colab GPU\",\n",
        "    \"production_ready\": new_acc >= 80\n",
        "}\n",
        "\n",
        "with open('/content/skin_model_info.json', 'w') as f:\n",
        "    json.dump(model_info, f, indent=2)\n",
        "\n",
        "print(\"✅ Model saved successfully!\")\n",
        "print(f\"📁 Files saved:\")\n",
        "print(f\"   - {final_model_path}\")\n",
        "print(f\"   - /content/skin_class_mapping.json\")\n",
        "print(f\"   - /content/skin_model_info.json\")\n",
        "\n",
        "# Download files\n",
        "print(\"\\n📥 Downloading files...\")\n",
        "try:\n",
        "    files.download('/content/skin_disease_resnet50_optimized.pth')\n",
        "    files.download('/content/skin_class_mapping.json')\n",
        "    files.download('/content/skin_model_info.json')\n",
        "    print(\"\\n🎉 All files downloaded successfully!\")\n",
        "    print(f\"\\n🚀 SKIN DISEASE MODEL OPTIMIZATION COMPLETE!\")\n",
        "    print(f\"📊 Improved from 44.83% to {new_acc:.2f}% (+{improvement:.1f}%)\")\n",
        "except Exception as e:\n",
        "    print(f\"Download error: {e}\")\n",
        "    print(\"You can manually download the files from the Files panel on the left\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "integration_guide"
      },
      "source": [
        "## 🔧 SwasthVedha Integration Guide\n",
        "\n",
        "### **Integration Steps:**\n",
        "\n",
        "1. **Place downloaded files** in your SwasthVedha project:\n",
        "   ```\n",
        "   SwasthVedha/backend/models/\n",
        "   ├── skin_disease_resnet50_optimized.pth\n",
        "   ├── skin_class_mapping.json\n",
        "   └── skin_model_info.json\n",
        "   ```\n",
        "\n",
        "2. **Update your backend code**:\n",
        "   ```python\n",
        "   import torch\n",
        "   from torchvision import transforms\n",
        "   import json\n",
        "\n",
        "   # Load optimized model\n",
        "   checkpoint = torch.load('models/skin_disease_resnet50_optimized.pth')\n",
        "   model.load_state_dict(checkpoint['model_state_dict'])\n",
        "   model.eval()\n",
        "\n",
        "   # Load classes\n",
        "   with open('models/skin_class_mapping.json', 'r') as f:\n",
        "       class_mapping = json.load(f)\n",
        "   ```\n",
        "\n",
        "3. **Expected Performance**:\n",
        "   - ✅ Significant improvement over 44.83%\n",
        "   - ✅ Production-ready if >80% accuracy\n",
        "   - ✅ Same quality as your Hair Disease model\n",
        "\n",
        "**🎯 Mission Complete: Transform your skin disease model from moderate to excellent performance!**\n"
      ]
    }
  ]
}