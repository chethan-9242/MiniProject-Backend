{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flan-T5 (Google) + GraphRAG (ChromaDB) â€” LoRA Training Notebook\n",
    "\n",
    "This Colab trains a LoRA adapter on `google/flan-t5-small` for personalized Ayurvedic recommendations and uses GraphRAG with ChromaDB for retrieval at inference. After training, download the adapter and place it in `backend/models/flan_t5_dosha_lora` in your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install \"transformers>=4.44.0\" datasets peft accelerate sentencepiece \"chromadb>=0.5.0\" \"sentence-transformers>=3.0.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, random, zipfile\n",
    "from typing import Dict, List\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Trainer, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model, PeftModel, prepare_model_for_kbit_training\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from chromadb.utils import embedding_functions\n",
    "from google.colab import files\n",
    "\n",
    "BASE_MODEL = \"google/flan-t5-small\"\n",
    "ADAPTER_OUT_DIR = \"/content/flan_t5_dosha_lora\"\n",
    "CHROMA_DIR = \"/content/chroma_db\"\n",
    "SEED = 42\n",
    "MAX_SOURCE_LEN = 512\n",
    "MAX_TARGET_LEN = 256\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ayurvedic Knowledge Base for GraphRAG\n",
    "AYURVEDA_KNOWLEDGE: List[Dict] = [\n",
    "    {\"id\": \"dosha_vata\", \"category\": \"Doshas\", \"title\": \"Vata Dosha Characteristics\",\n",
    "     \"content\": \"Vata governs movement, circulation, breathing, and nerve impulses. Imbalance -> anxiety, dry skin, constipation.\",\n",
    "     \"metadata\": {\"type\": \"constitution\", \"dosha\": \"vata\"}},\n",
    "    {\"id\": \"dosha_pitta\", \"category\": \"Doshas\", \"title\": \"Pitta Dosha Characteristics\",\n",
    "     \"content\": \"Pitta governs digestion, metabolism, temperature. Imbalance -> inflammation, anger, heartburn, skin issues.\",\n",
    "     \"metadata\": {\"type\": \"constitution\", \"dosha\": \"pitta\"}},\n",
    "    {\"id\": \"dosha_kapha\", \"category\": \"Doshas\", \"title\": \"Kapha Dosha Characteristics\",\n",
    "     \"content\": \"Kapha governs structure, lubrication, stability. Imbalance -> weight gain, lethargy, congestion.\",\n",
    "     \"metadata\": {\"type\": \"constitution\", \"dosha\": \"kapha\"}},\n",
    "    {\"id\": \"herb_ashwagandha\", \"category\": \"Herbs\", \"title\": \"Ashwagandha\",\n",
    "     \"content\": \"Adaptogen: reduces stress/anxiety, improves sleep and cognition. Balances Vata/Kapha.\",\n",
    "     \"metadata\": {\"type\": \"herb\"}},\n",
    "    {\"id\": \"herb_turmeric\", \"category\": \"Herbs\", \"title\": \"Turmeric\",\n",
    "     \"content\": \"Anti-inflammatory/antioxidant; supports skin, joints, liver. Works across doshas.\",\n",
    "     \"metadata\": {\"type\": \"herb\"}},\n",
    "    {\"id\": \"treatment_skin_pitta\", \"category\": \"Treatments\", \"title\": \"Pitta Skin Treatment\",\n",
    "     \"content\": \"Cooling herbs (neem, aloe), avoid hot/spicy/acidic foods, use coconut oil, cooling foods.\",\n",
    "     \"metadata\": {\"type\": \"treatment\", \"dosha\": \"pitta\", \"condition\": \"skin\"}},\n",
    "    {\"id\": \"diet_vata\", \"category\": \"Diet\", \"title\": \"Vata Diet\",\n",
    "     \"content\": \"Warm, moist, grounding foods; ghee; root vegetables; sweet/sour/salty tastes; avoid cold/dry.\",\n",
    "     \"metadata\": {\"type\": \"diet\", \"dosha\": \"vata\"}},\n",
    "    {\"id\": \"diet_pitta\", \"category\": \"Diet\", \"title\": \"Pitta Diet\",\n",
    "     \"content\": \"Cool, hydrating, mildly spiced; coconut, cucumber; avoid hot/fried/acidic foods.\",\n",
    "     \"metadata\": {\"type\": \"diet\", \"dosha\": \"pitta\"}},\n",
    "    {\"id\": \"diet_kapha\", \"category\": \"Diet\", \"title\": \"Kapha Diet\",\n",
    "     \"content\": \"Light, warm, stimulating; pungent/bitter/astringent; reduce heavy/oily/sweet.\",\n",
    "     \"metadata\": {\"type\": \"diet\", \"dosha\": \"kapha\"}},\n",
    "    {\"id\": \"practice_abhyanga\", \"category\": \"Practices\", \"title\": \"Abhyanga\",\n",
    "     \"content\": \"Daily self-massage: sesame oil (Vata), coconut (Pitta), mustard/sunflower (Kapha).\",\n",
    "     \"metadata\": {\"type\": \"practice\"}},\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ChromaDB and create collection if missing\n",
    "def init_chromadb(persist_dir: str = CHROMA_DIR):\n",
    "    os.makedirs(persist_dir, exist_ok=True)\n",
    "    client = chromadb.Client(Settings(persist_directory=persist_dir, anonymized_telemetry=False))\n",
    "    try:\n",
    "        coll = client.get_collection('ayurveda_knowledge')\n",
    "    except Exception:\n",
    "        ef = embedding_functions.SentenceTransformerEmbeddingFunction(model_name='all-MiniLM-L6-v2')\n",
    "        coll = client.create_collection(name='ayurveda_knowledge', embedding_function=ef,\n",
    "                                       metadata={'description': 'Ayurveda KB for RAG'})\n",
    "        coll.add(\n",
    "            documents=[d['content'] for d in AYURVEDA_KNOWLEDGE],\n",
    "            metadatas=[d['metadata'] for d in AYURVEDA_KNOWLEDGE],\n",
    "            ids=[d['id'] for d in AYURVEDA_KNOWLEDGE],\n",
    "        )\n",
    "    return client, coll\n",
    "\n",
    "client, collection = init_chromadb()\n",
    "print('ChromaDB docs:', collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload JSONL dataset (fields: input, output). If skipped, demo data is used.\n",
    "DEMO_DATA = [\n",
    "    {\n",
    "        'input': 'Dosha: Vata\\nSymptoms: anxiety, dry skin, constipation\\nContext: adult, moderate',\n",
    "        'output': 'Health:\\n- Maintain warm routine and regular schedule.\\nDiet:\\n- Warm, moist foods with ghee.\\nLifestyle:\\n- Abhyanga with sesame oil.\\nWarnings:\\n- If constipation persists > 2 weeks, consult doctor.'\n",
    "    },\n",
    "    {\n",
    "        'input': 'Dosha: Pitta\\nSymptoms: rash, burning sensation, irritability\\nContext: adult, mild',\n",
    "        'output': 'Health:\\n- Reduce heat and stress.\\nDiet:\\n- Cooling foods (cucumber, coconut).\\nLifestyle:\\n- Avoid midday sun; use aloe.\\nWarnings:\\n- If rash spreads rapidly, seek care.'\n",
    "    },\n",
    "    {\n",
    "        'input': 'Dosha: Kapha\\nSymptoms: fatigue, congestion, weight gain\\nContext: adult, mild',\n",
    "        'output': 'Health:\\n- Daily vigorous exercise.\\nDiet:\\n- Light, warm, pungent foods.\\nLifestyle:\\n- Wake up before 6 AM.\\nWarnings:\\n- If breathlessness or edema, consult physician.'\n",
    "    },\n",
    "]\n",
    "\n",
    "uploaded = files.upload()\n",
    "if uploaded:\n",
    "    fname = list(uploaded.keys())[0]\n",
    "    rows = []\n",
    "    with open(fname, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            obj = json.loads(line)\n",
    "            rows.append({'input': obj['input'], 'target': obj['output']})\n",
    "    ds = Dataset.from_list(rows)\n",
    "else:\n",
    "    ds = Dataset.from_list(DEMO_DATA)\n",
    "\n",
    "def to_prompt(ex):\n",
    "    return {\n",
    "        'input_text': (\n",
    "            'You are an Ayurvedic assistant. Generate concise, specific, non-repetitive recommendations.\\n'\n",
    "            'Return sections: Health, Diet, Lifestyle, Warnings.\\n\\n'\n",
    "            + ex['input'] + \"\\n\\nAnswer:\"\n",
    "        ),\n",
    "        'labels': ex['target']\n",
    "    }\n",
    "\n",
    "ds = ds.map(to_prompt)\n",
    "split = ds.train_test_split(test_size=0.1, seed=SEED)\n",
    "train_ds, val_ds = split['train'], split['test']\n",
    "len(train_ds), len(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Flan-T5-small and prepare LoRA\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "base = AutoModelForSeq2SeqLM.from_pretrained(BASE_MODEL)\n",
    "base = prepare_model_for_kbit_training(base)\n",
    "lora_cfg = LoraConfig(r=16, lora_alpha=32, target_modules=['q','k','v','o'], lora_dropout=0.05, bias='none', task_type='SEQ_2_SEQ_LM')\n",
    "model = get_peft_model(base, lora_cfg).to(device)\n",
    "\n",
    "def tokenize_batch(batch):\n",
    "    mi = tokenizer(batch['input_text'], max_length=MAX_SOURCE_LEN, truncation=True)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(batch['labels'], max_length=MAX_TARGET_LEN, truncation=True)\n",
    "    mi['labels'] = labels['input_ids']\n",
    "    return mi\n",
    "\n",
    "collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "train_tok = train_ds.map(tokenize_batch, batched=True, remove_columns=train_ds.column_names)\n",
    "val_tok = val_ds.map(tokenize_batch, batched=True, remove_columns=val_ds.column_names)\n",
    "len(train_tok), len(val_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=ADAPTER_OUT_DIR,\n",
    "    per_device_train_batch_size=8, per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=2, learning_rate=2e-4, num_train_epochs=3,\n",
    "    eval_strategy='steps', logging_steps=50, eval_steps=200, save_steps=200, save_total_limit=2,\n",
    "    lr_scheduler_type='cosine', warmup_ratio=0.03, weight_decay=0.01,\n",
    "    bf16=torch.cuda.is_available(), fp16=not torch.cuda.is_available(), report_to='none'\n",
    ")\n",
    "trainer = Trainer(model=model, args=args, train_dataset=train_tok, eval_dataset=val_tok, tokenizer=tokenizer, data_collator=collator)\n",
    "trainer.train()\n",
    "model.save_pretrained(ADAPTER_OUT_DIR); tokenizer.save_pretrained(ADAPTER_OUT_DIR)\n",
    "print('Saved adapter at', ADAPTER_OUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GraphRAG: retrieve + generate\n",
    "def retrieve_context(query: str, n_results: int = 5) -> List[str]:\n",
    "    res = collection.query(query_texts=[query], n_results=n_results)\n",
    "    return (res.get('documents', [[]])[0]) or []\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_with_rag(user_input: str, max_len: int = 280) -> str:\n",
    "    ctx_docs = retrieve_context(user_input, n_results=5)\n",
    "    context = '\\n\\n'.join([f'Source {i+1}: {d}' for i, d in enumerate(ctx_docs)])\n",
    "    prompt = (\n",
    "        'Based on the following Ayurvedic knowledge, generate concise, specific recommendations.\\n'\n",
    "        'Return sections: Health, Diet, Lifestyle, Warnings.\\n\\n'\n",
    "        f'Context:\\n{context}\\n\\nInput:\\n{user_input}\\n\\nAnswer:'\n",
    "    )\n",
    "    inputs = tokenizer(prompt, return_tensors='pt', max_length=MAX_SOURCE_LEN, truncation=True).to(device)\n",
    "    out = model.generate(**inputs, max_length=max_len, min_length=80, num_beams=5, early_stopping=True,\n",
    "                         temperature=0.9, do_sample=True, top_k=60, top_p=0.92,\n",
    "                         repetition_penalty=1.25, no_repeat_ngram_size=3)\n",
    "    return tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "demo_input = 'Dosha: Vata-Pitta\\nSymptoms: bloating, anxiety\\nContext: adult, moderate severity'\n",
    "print(generate_with_rag(demo_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip adapter for download\n",
    "zip_path = '/content/flan_t5_dosha_lora.zip'\n",
    "import os\n",
    "with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zf:\n",
    "    for root, _, files in os.walk(ADAPTER_OUT_DIR):\n",
    "        for f in files:\n",
    "            full = os.path.join(root, f)\n",
    "            rel = os.path.relpath(full, ADAPTER_OUT_DIR)\n",
    "            zf.write(full, arcname=os.path.join('flan_t5_dosha_lora', rel))\n",
    "print('Download from left pane:', zip_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
