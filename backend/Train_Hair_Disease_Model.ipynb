{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üíá SwasthVedha - Hair Disease Detection Model Training\n",
        "\n",
        "**Dataset:** Hair Diseases (Kaggle)\n",
        "\n",
        "**Goal:** Train a ResNet50 CNN model that:\n",
        "- ‚úÖ Learns patterns from all hair disease categories\n",
        "- ‚úÖ Prevents overfitting (dropout, augmentation)\n",
        "- ‚úÖ Prevents underfitting (proper training)\n",
        "- ‚úÖ Uses transfer learning (ResNet50)\n",
        "- ‚úÖ Detects uncertainty (confidence thresholds)\n",
        "\n",
        "**Target Accuracy:** 85-95%"
      ],
      "metadata": {
        "id": "title"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üì¶ Step 1: Setup & Install Dependencies"
      ],
      "metadata": {
        "id": "setup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install -q kaggle torch torchvision\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "from pathlib import Path\n",
        "import json\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "# Check GPU\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
      ],
      "metadata": {
        "id": "install"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîë Step 2: Kaggle API Setup\n",
        "\n",
        "**Upload your `kaggle.json` file below**"
      ],
      "metadata": {
        "id": "kaggle_setup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Upload kaggle.json\n",
        "print(\"Please upload your kaggle.json file:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Setup Kaggle credentials\n",
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "print(\"‚úÖ Kaggle API configured!\")"
      ],
      "metadata": {
        "id": "kaggle_auth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üì• Step 3: Download Dataset"
      ],
      "metadata": {
        "id": "download"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download Hair Diseases dataset from Kaggle\n",
        "!kaggle datasets download -d sundarannamalai/hair-diseases\n",
        "\n",
        "# Extract\n",
        "!unzip -q hair-diseases.zip -d dataset\n",
        "\n",
        "# Check structure\n",
        "!ls -lh dataset/\n",
        "\n",
        "print(\"\\n‚úÖ Dataset downloaded and extracted!\")"
      ],
      "metadata": {
        "id": "download_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîç Step 4: Analyze Dataset Structure"
      ],
      "metadata": {
        "id": "analyze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the data directory\n",
        "data_root = Path('dataset')\n",
        "\n",
        "# Look for train/test folders\n",
        "possible_paths = list(data_root.rglob('*'))\n",
        "print(\"Dataset structure (first 30 items):\")\n",
        "for p in sorted(possible_paths)[:30]:\n",
        "    print(f\"  {p}\")\n",
        "\n",
        "# Find actual data directory\n",
        "data_dirs = [p for p in data_root.rglob('*') if p.is_dir() and any((p / subdir).exists() for subdir in ['train', 'test', 'Train', 'Test', 'training', 'testing'])]\n",
        "\n",
        "if data_dirs:\n",
        "    data_dir = data_dirs[0]\n",
        "    print(f\"\\n‚úÖ Found data directory: {data_dir}\")\n",
        "else:\n",
        "    # Check for direct class folders\n",
        "    class_folders = [p for p in data_root.iterdir() if p.is_dir() and len(list(p.glob('*.jpg'))) > 0]\n",
        "    if class_folders:\n",
        "        data_dir = data_root\n",
        "        print(f\"\\n‚úÖ Found class folders directly in: {data_dir}\")\n",
        "    else:\n",
        "        data_dir = data_root\n",
        "        print(f\"\\n‚ö†Ô∏è Using root directory: {data_dir}\")\n",
        "\n",
        "# Check for train/test folders\n",
        "train_dir = None\n",
        "test_dir = None\n",
        "\n",
        "for variant in ['train', 'Train', 'training', 'Training', 'train_set']:\n",
        "    potential = data_dir / variant\n",
        "    if potential.exists():\n",
        "        train_dir = potential\n",
        "        print(f\"‚úÖ Found training data: {train_dir}\")\n",
        "        break\n",
        "\n",
        "for variant in ['test', 'Test', 'testing', 'Testing', 'test_set', 'val', 'validation']:\n",
        "    potential = data_dir / variant\n",
        "    if potential.exists():\n",
        "        test_dir = potential\n",
        "        print(f\"‚úÖ Found test data: {test_dir}\")\n",
        "        break\n",
        "\n",
        "# If no train/test split, use all data as training\n",
        "if not train_dir:\n",
        "    # Check if data_dir itself contains class folders\n",
        "    class_folders = [d for d in data_dir.iterdir() if d.is_dir()]\n",
        "    if class_folders:\n",
        "        train_dir = data_dir\n",
        "        print(f\"\\n‚ö†Ô∏è No train/test split found. Using all data from: {train_dir}\")\n",
        "        print(\"   Will create 80/20 split during training\")\n",
        "\n",
        "# List classes\n",
        "if train_dir and train_dir.exists():\n",
        "    classes = sorted([d.name for d in train_dir.iterdir() if d.is_dir()])\n",
        "    print(f\"\\nüìä Found {len(classes)} hair disease classes:\")\n",
        "    total_images = 0\n",
        "    for i, cls in enumerate(classes, 1):\n",
        "        num_images = len(list((train_dir / cls).glob('*.[jp][pn][g]*'))) + \\\n",
        "                     len(list((train_dir / cls).glob('*.jpeg'))) + \\\n",
        "                     len(list((train_dir / cls).glob('*.webp')))\n",
        "        total_images += num_images\n",
        "        print(f\"  {i}. {cls}: {num_images} images\")\n",
        "    print(f\"\\n‚úÖ Total images: {total_images}\")\n",
        "else:\n",
        "    print(\"\\n‚ùå ERROR: Could not find training data!\")"
      ],
      "metadata": {
        "id": "analyze_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîÑ Step 5: Data Augmentation & Preprocessing"
      ],
      "metadata": {
        "id": "augmentation"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Advanced data augmentation (prevents overfitting)\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize((256, 256)),\n",
        "        transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomVerticalFlip(p=0.3),\n",
        "        transforms.RandomRotation(20),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "        transforms.RandomErasing(p=0.2, scale=(0.02, 0.15))\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize((256, 256)),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "print(\"‚úÖ Data augmentation configured!\")\n",
        "print(\"   - Random crops, flips, rotations\")\n",
        "print(\"   - Color jittering\")\n",
        "print(\"   - Random erasing\")\n",
        "print(\"   ‚Üí This prevents overfitting!\")"
      ],
      "metadata": {
        "id": "augmentation_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìä Step 6: Create Data Loaders"
      ],
      "metadata": {
        "id": "loaders"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create datasets\n",
        "if test_dir and test_dir.exists():\n",
        "    # Separate train/test\n",
        "    image_datasets = {\n",
        "        'train': datasets.ImageFolder(train_dir, data_transforms['train']),\n",
        "        'val': datasets.ImageFolder(test_dir, data_transforms['val'])\n",
        "    }\n",
        "else:\n",
        "    # Split training set\n",
        "    full_dataset = datasets.ImageFolder(train_dir, data_transforms['train'])\n",
        "    train_size = int(0.8 * len(full_dataset))\n",
        "    val_size = len(full_dataset) - train_size\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
        "    \n",
        "    # Apply validation transforms to val set\n",
        "    val_dataset.dataset.transform = data_transforms['val']\n",
        "    \n",
        "    image_datasets = {\n",
        "        'train': train_dataset,\n",
        "        'val': val_dataset\n",
        "    }\n",
        "\n",
        "# Create data loaders\n",
        "dataloaders = {\n",
        "    'train': torch.utils.data.DataLoader(image_datasets['train'], batch_size=32, shuffle=True, num_workers=2),\n",
        "    'val': torch.utils.data.DataLoader(image_datasets['val'], batch_size=32, shuffle=False, num_workers=2)\n",
        "}\n",
        "\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "class_names = image_datasets['train'].dataset.classes if hasattr(image_datasets['train'], 'dataset') else image_datasets['train'].classes\n",
        "num_classes = len(class_names)\n",
        "\n",
        "print(f\"‚úÖ Data loaders created!\")\n",
        "print(f\"   Training samples: {dataset_sizes['train']}\")\n",
        "print(f\"   Validation samples: {dataset_sizes['val']}\")\n",
        "print(f\"   Number of classes: {num_classes}\")\n",
        "print(f\"   Classes: {class_names}\")"
      ],
      "metadata": {
        "id": "loaders_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üèóÔ∏è Step 7: Build ResNet50 Model"
      ],
      "metadata": {
        "id": "model"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create model with transfer learning (ResNet50)\n",
        "def create_model(num_classes):\n",
        "    # Load pre-trained ResNet50\n",
        "    model = models.resnet50(weights='IMAGENET1K_V2')\n",
        "    \n",
        "    # Freeze early layers (keep ImageNet knowledge)\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "    \n",
        "    # Replace final classifier with custom head\n",
        "    num_features = model.fc.in_features\n",
        "    model.fc = nn.Sequential(\n",
        "        nn.Dropout(0.5),  # Prevents overfitting\n",
        "        nn.Linear(num_features, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.BatchNorm1d(512),\n",
        "        nn.Dropout(0.3),  # More dropout = less overfitting\n",
        "        nn.Linear(512, num_classes)\n",
        "    )\n",
        "    \n",
        "    return model\n",
        "\n",
        "model = create_model(num_classes)\n",
        "model = model.to(device)\n",
        "\n",
        "print(\"‚úÖ Model created!\")\n",
        "print(f\"   Architecture: ResNet50\")\n",
        "print(f\"   Transfer Learning: Yes (ImageNet pre-trained)\")\n",
        "print(f\"   Dropout: 0.5 and 0.3 (prevents overfitting)\")\n",
        "print(f\"   Output classes: {num_classes}\")"
      ],
      "metadata": {
        "id": "model_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üéØ Step 8: Training Configuration"
      ],
      "metadata": {
        "id": "training_config"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer (Adam)\n",
        "optimizer = optim.Adam(model.fc.parameters(), lr=0.001, weight_decay=0.0001)\n",
        "\n",
        "# Learning rate scheduler (prevents underfitting)\n",
        "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3)\n",
        "\n",
        "print(\"‚úÖ Training configuration:\")\n",
        "print(f\"   Loss: CrossEntropyLoss\")\n",
        "print(f\"   Optimizer: Adam\")\n",
        "print(f\"   Learning Rate: 0.001\")\n",
        "print(f\"   Scheduler: ReduceLROnPlateau\")\n",
        "print(f\"   Epochs: 50 (with early stopping)\")"
      ],
      "metadata": {
        "id": "training_config_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üöÄ Step 9: Training Loop"
      ],
      "metadata": {
        "id": "training"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=50):\n",
        "    since = time.time()\n",
        "    \n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "    patience_counter = 0\n",
        "    patience = 10  # Early stopping patience\n",
        "    \n",
        "    history = {\n",
        "        'train_loss': [], 'train_acc': [],\n",
        "        'val_loss': [], 'val_acc': []\n",
        "    }\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
        "        print('-' * 50)\n",
        "        \n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "            \n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "            \n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                \n",
        "                optimizer.zero_grad()\n",
        "                \n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    \n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "                \n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "            \n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "            \n",
        "            history[f'{phase}_loss'].append(epoch_loss)\n",
        "            history[f'{phase}_acc'].append(epoch_acc.item())\n",
        "            \n",
        "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "            \n",
        "            if phase == 'val':\n",
        "                scheduler.step(epoch_acc)\n",
        "                \n",
        "                if epoch_acc > best_acc:\n",
        "                    best_acc = epoch_acc\n",
        "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                    patience_counter = 0\n",
        "                    print(f'‚úÖ New best model! Acc: {best_acc:.4f}')\n",
        "                else:\n",
        "                    patience_counter += 1\n",
        "        \n",
        "        if patience_counter >= patience:\n",
        "            print(f'\\n‚ö†Ô∏è Early stopping at epoch {epoch+1}')\n",
        "            break\n",
        "    \n",
        "    time_elapsed = time.time() - since\n",
        "    print(f'\\n‚úÖ Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "    print(f'   Best validation Acc: {best_acc:.4f}')\n",
        "    \n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, history\n",
        "\n",
        "# Train the model\n",
        "model, history = train_model(model, criterion, optimizer, scheduler, num_epochs=50)"
      ],
      "metadata": {
        "id": "training_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìà Step 10: Visualize Training"
      ],
      "metadata": {
        "id": "visualize"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training history\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history['train_acc'], label='Train')\n",
        "plt.plot(history['val_acc'], label='Validation')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history['train_loss'], label='Train')\n",
        "plt.plot(history['val_loss'], label='Validation')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüìä Training Analysis:\")\n",
        "print(f\"   Final Train Acc: {history['train_acc'][-1]:.4f}\")\n",
        "print(f\"   Final Val Acc: {history['val_acc'][-1]:.4f}\")\n",
        "gap = history['train_acc'][-1] - history['val_acc'][-1]\n",
        "print(f\"   Accuracy Gap: {gap:.4f}\")\n",
        "if gap < 0.1:\n",
        "    print(\"   ‚úÖ Good! No overfitting detected\")\n",
        "elif gap < 0.2:\n",
        "    print(\"   ‚ö†Ô∏è Slight overfitting\")\n",
        "else:\n",
        "    print(\"   ‚ùå Overfitting detected!\")"
      ],
      "metadata": {
        "id": "visualize_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üíæ Step 11: Save Model"
      ],
      "metadata": {
        "id": "save"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model with metadata\n",
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'model_architecture': 'ResNet50',\n",
        "    'num_classes': num_classes,\n",
        "    'class_names': class_names,\n",
        "    'best_val_accuracy': history['val_acc'][-1],\n",
        "    'training_history': history,\n",
        "    'model_info': {\n",
        "        'architecture': 'ResNet50 with Transfer Learning',\n",
        "        'input_size': (224, 224),\n",
        "        'num_classes': num_classes,\n",
        "        'dropout_rate': 0.5,\n",
        "        'regularization': 'Dropout + BatchNorm + Data Augmentation',\n",
        "        'best_validation_accuracy': history['val_acc'][-1]\n",
        "    }\n",
        "}, 'hair_classifier.pth')\n",
        "\n",
        "# Save class mapping\n",
        "class_mapping = {str(i): name for i, name in enumerate(class_names)}\n",
        "with open('hair_classes.json', 'w') as f:\n",
        "    json.dump(class_mapping, f, indent=2)\n",
        "\n",
        "print(\"‚úÖ Model saved!\")\n",
        "print(\"   Files: hair_classifier.pth, hair_classes.json\")\n",
        "\n",
        "# Download files\n",
        "from google.colab import files\n",
        "files.download('hair_classifier.pth')\n",
        "files.download('hair_classes.json')\n",
        "\n",
        "print(\"\\nüì• Files downloaded! Copy them to your backend/models/ folder\")"
      ],
      "metadata": {
        "id": "save_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úÖ Training Complete!\n",
        "\n",
        "### Next Steps:\n",
        "\n",
        "1. **Download the model files** (done above)\n",
        "2. **Copy to your backend:**\n",
        "   ```bash\n",
        "   backend/models/hair_classifier.pth\n",
        "   backend/models/hair_classes.json\n",
        "   ```\n",
        "3. **Create API endpoint** for hair disease predictions\n",
        "4. **Test with real images**\n",
        "\n",
        "### Model Features:\n",
        "\n",
        "‚úÖ ResNet50 with transfer learning  \n",
        "‚úÖ Dropout (prevents overfitting)  \n",
        "‚úÖ Data augmentation  \n",
        "‚úÖ Early stopping  \n",
        "‚úÖ Learning rate scheduler  \n",
        "\n",
        "**Target Accuracy:** 85-95% üéØ"
      ],
      "metadata": {
        "id": "complete"
      }
    }
  ]
}
