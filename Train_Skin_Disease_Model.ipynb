{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ü©∫ SwasthVedha - Skin Disease Detection Model Training\n",
        "\n",
        "**Dataset:** Skin-Disease-Dataset (Kaggle)\n",
        "\n",
        "**Goal:** Train a high-accuracy CNN model that:\n",
        "- ‚úÖ Learns patterns from all diseases in dataset\n",
        "- ‚úÖ Prevents overfitting (dropout, augmentation)\n",
        "- ‚úÖ Prevents underfitting (proper training)\n",
        "- ‚úÖ Uses transfer learning (EfficientNetB0)\n",
        "- ‚úÖ Detects uncertainty (confidence thresholds)\n",
        "- ‚úÖ Flags unknown cases\n",
        "\n",
        "**Target Accuracy:** 85-95%"
      ],
      "metadata": {
        "id": "title"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üì¶ Step 1: Setup & Install Dependencies"
      ],
      "metadata": {
        "id": "setup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install -q kaggle torch torchvision efficientnet_pytorch\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "from pathlib import Path\n",
        "import json\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "# Check GPU\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
      ],
      "metadata": {
        "id": "install"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîë Step 2: Kaggle API Setup\n",
        "\n",
        "**Instructions:**\n",
        "1. Go to https://www.kaggle.com/settings\n",
        "2. Scroll to \"API\" section\n",
        "3. Click \"Create New API Token\"\n",
        "4. Upload the `kaggle.json` file below"
      ],
      "metadata": {
        "id": "kaggle_setup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Upload kaggle.json\n",
        "print(\"Please upload your kaggle.json file:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Setup Kaggle credentials\n",
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "print(\"‚úÖ Kaggle API configured!\")"
      ],
      "metadata": {
        "id": "kaggle_auth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üì• Step 3: Download Dataset"
      ],
      "metadata": {
        "id": "download"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download Skin-Disease-Dataset from Kaggle\n",
        "!kaggle datasets download -d subirbiswas19/skin-disease-dataset\n",
        "\n",
        "# Extract\n",
        "!unzip -q skin-disease-dataset.zip -d dataset\n",
        "\n",
        "# Check structure\n",
        "!ls -lh dataset/\n",
        "\n",
        "print(\"\\n‚úÖ Dataset downloaded and extracted!\")"
      ],
      "metadata": {
        "id": "download_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîç Step 4: Analyze Dataset Structure"
      ],
      "metadata": {
        "id": "analyze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the data directory\n",
        "data_root = Path('dataset')\n",
        "\n",
        "# Look for train/test folders\n",
        "possible_paths = list(data_root.rglob('*'))\n",
        "print(\"Dataset structure:\")\n",
        "for p in sorted(possible_paths)[:20]:  # Show first 20\n",
        "    print(f\"  {p}\")\n",
        "\n",
        "# Find actual data directory\n",
        "data_dirs = [p for p in data_root.rglob('*') if p.is_dir() and any((p / subdir).exists() for subdir in ['train', 'test', 'Train', 'Test'])]\n",
        "\n",
        "if data_dirs:\n",
        "    data_dir = data_dirs[0]\n",
        "    print(f\"\\n‚úÖ Found data directory: {data_dir}\")\n",
        "else:\n",
        "    # Assume top level\n",
        "    data_dir = data_root\n",
        "    print(f\"\\n‚ö†Ô∏è Using root directory: {data_dir}\")\n",
        "\n",
        "# Check for train/test folders (handle different naming)\n",
        "train_dir = None\n",
        "test_dir = None\n",
        "\n",
        "for variant in ['train', 'Train', 'training', 'Training']:\n",
        "    if (data_dir / variant).exists():\n",
        "        train_dir = data_dir / variant\n",
        "        break\n",
        "\n",
        "for variant in ['test', 'Test', 'testing', 'Testing', 'val', 'validation']:\n",
        "    if (data_dir / variant).exists():\n",
        "        test_dir = data_dir / variant\n",
        "        break\n",
        "\n",
        "print(f\"\\nTrain directory: {train_dir}\")\n",
        "print(f\"Test directory: {test_dir}\")\n",
        "\n",
        "# List classes\n",
        "if train_dir and train_dir.exists():\n",
        "    classes = sorted([d.name for d in train_dir.iterdir() if d.is_dir()])\n",
        "    print(f\"\\nüìä Found {len(classes)} disease classes:\")\n",
        "    for i, cls in enumerate(classes, 1):\n",
        "        num_images = len(list((train_dir / cls).glob('*')))\n",
        "        print(f\"  {i}. {cls}: {num_images} images\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è Could not find train directory. Please check dataset structure.\")"
      ],
      "metadata": {
        "id": "analyze_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîÑ Step 5: Data Augmentation & Preprocessing\n",
        "\n",
        "**Prevents Overfitting with:**\n",
        "- Random flips, rotations\n",
        "- Color jittering\n",
        "- Random erasing"
      ],
      "metadata": {
        "id": "augmentation"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Advanced data augmentation (prevents overfitting)\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize((256, 256)),\n",
        "        transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomVerticalFlip(p=0.3),\n",
        "        transforms.RandomRotation(20),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "        transforms.RandomErasing(p=0.2, scale=(0.02, 0.15))\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize((256, 256)),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "print(\"‚úÖ Data augmentation configured!\")\n",
        "print(\"   - Random crops, flips, rotations\")\n",
        "print(\"   - Color jittering\")\n",
        "print(\"   - Random erasing\")\n",
        "print(\"   ‚Üí This prevents overfitting!\")"
      ],
      "metadata": {
        "id": "augmentation_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìä Step 6: Create Data Loaders"
      ],
      "metadata": {
        "id": "loaders"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create datasets\n",
        "if test_dir and test_dir.exists():\n",
        "    # Separate train/test\n",
        "    image_datasets = {\n",
        "        'train': datasets.ImageFolder(train_dir, data_transforms['train']),\n",
        "        'val': datasets.ImageFolder(test_dir, data_transforms['val'])\n",
        "    }\n",
        "else:\n",
        "    # Split training set\n",
        "    full_dataset = datasets.ImageFolder(train_dir, data_transforms['train'])\n",
        "    train_size = int(0.8 * len(full_dataset))\n",
        "    val_size = len(full_dataset) - train_size\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
        "    \n",
        "    # Apply validation transforms to val set\n",
        "    val_dataset.dataset.transform = data_transforms['val']\n",
        "    \n",
        "    image_datasets = {\n",
        "        'train': train_dataset,\n",
        "        'val': val_dataset\n",
        "    }\n",
        "\n",
        "# Create data loaders\n",
        "dataloaders = {\n",
        "    'train': torch.utils.data.DataLoader(image_datasets['train'], batch_size=32, shuffle=True, num_workers=2),\n",
        "    'val': torch.utils.data.DataLoader(image_datasets['val'], batch_size=32, shuffle=False, num_workers=2)\n",
        "}\n",
        "\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "class_names = image_datasets['train'].dataset.classes if hasattr(image_datasets['train'], 'dataset') else image_datasets['train'].classes\n",
        "num_classes = len(class_names)\n",
        "\n",
        "print(f\"‚úÖ Data loaders created!\")\n",
        "print(f\"   Training samples: {dataset_sizes['train']}\")\n",
        "print(f\"   Validation samples: {dataset_sizes['val']}\")\n",
        "print(f\"   Number of classes: {num_classes}\")\n",
        "print(f\"   Classes: {class_names}\")"
      ],
      "metadata": {
        "id": "loaders_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üèóÔ∏è Step 7: Build Model (Transfer Learning)\n",
        "\n",
        "**Using ResNet50 with Transfer Learning:**\n",
        "- Pre-trained on ImageNet (1M images)\n",
        "- Already knows patterns, textures, shapes\n",
        "- We fine-tune for skin diseases"
      ],
      "metadata": {
        "id": "model"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create model with transfer learning\n",
        "def create_model(num_classes):\n",
        "    # Load pre-trained ResNet50\n",
        "    model = models.resnet50(weights='IMAGENET1K_V2')\n",
        "    \n",
        "    # Freeze early layers (keep ImageNet knowledge)\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "    \n",
        "    # Replace final classifier with custom head\n",
        "    num_features = model.fc.in_features\n",
        "    model.fc = nn.Sequential(\n",
        "        nn.Dropout(0.5),  # Prevents overfitting\n",
        "        nn.Linear(num_features, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.BatchNorm1d(512),\n",
        "        nn.Dropout(0.3),  # More dropout = less overfitting\n",
        "        nn.Linear(512, num_classes)\n",
        "    )\n",
        "    \n",
        "    return model\n",
        "\n",
        "model = create_model(num_classes)\n",
        "model = model.to(device)\n",
        "\n",
        "print(\"‚úÖ Model created!\")\n",
        "print(f\"   Architecture: ResNet50\")\n",
        "print(f\"   Transfer Learning: Yes (ImageNet pre-trained)\")\n",
        "print(f\"   Dropout: 0.5 and 0.3 (prevents overfitting)\")\n",
        "print(f\"   Output classes: {num_classes}\")"
      ],
      "metadata": {
        "id": "model_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üéØ Step 8: Training Configuration\n",
        "\n",
        "**Prevents Underfitting:**\n",
        "- Proper learning rate (0.001)\n",
        "- Learning rate scheduler\n",
        "- Enough epochs (50)"
      ],
      "metadata": {
        "id": "training_config"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer (Adam is good for medical images)\n",
        "optimizer = optim.Adam(model.fc.parameters(), lr=0.001, weight_decay=0.0001)\n",
        "\n",
        "# Learning rate scheduler (prevents underfitting)\n",
        "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3, verbose=True)\n",
        "\n",
        "print(\"‚úÖ Training configuration:\")\n",
        "print(f\"   Loss: CrossEntropyLoss\")\n",
        "print(f\"   Optimizer: Adam\")\n",
        "print(f\"   Learning Rate: 0.001\")\n",
        "print(f\"   Scheduler: ReduceLROnPlateau (prevents underfitting)\")\n",
        "print(f\"   Epochs: 50\")"
      ],
      "metadata": {
        "id": "training_config_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üöÄ Step 9: Training Loop\n",
        "\n",
        "**With Early Stopping** to prevent overfitting"
      ],
      "metadata": {
        "id": "training"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=50):\n",
        "    since = time.time()\n",
        "    \n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "    patience_counter = 0\n",
        "    patience = 10  # Early stopping patience\n",
        "    \n",
        "    history = {\n",
        "        'train_loss': [], 'train_acc': [],\n",
        "        'val_loss': [], 'val_acc': []\n",
        "    }\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
        "        print('-' * 50)\n",
        "        \n",
        "        # Each epoch has training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "            \n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "            \n",
        "            # Iterate over data\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                \n",
        "                optimizer.zero_grad()\n",
        "                \n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    \n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "                \n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "            \n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "            \n",
        "            history[f'{phase}_loss'].append(epoch_loss)\n",
        "            history[f'{phase}_acc'].append(epoch_acc.item())\n",
        "            \n",
        "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "            \n",
        "            # Save best model\n",
        "            if phase == 'val':\n",
        "                scheduler.step(epoch_acc)\n",
        "                \n",
        "                if epoch_acc > best_acc:\n",
        "                    best_acc = epoch_acc\n",
        "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                    patience_counter = 0\n",
        "                    print(f'‚úÖ New best model! Acc: {best_acc:.4f}')\n",
        "                else:\n",
        "                    patience_counter += 1\n",
        "        \n",
        "        # Early stopping\n",
        "        if patience_counter >= patience:\n",
        "            print(f'\\n‚ö†Ô∏è Early stopping at epoch {epoch+1}')\n",
        "            print(f'   Validation accuracy hasn\\'t improved for {patience} epochs')\n",
        "            break\n",
        "    \n",
        "    time_elapsed = time.time() - since\n",
        "    print(f'\\n‚úÖ Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "    print(f'   Best validation Acc: {best_acc:.4f}')\n",
        "    \n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, history\n",
        "\n",
        "# Train the model\n",
        "model, history = train_model(model, criterion, optimizer, scheduler, num_epochs=50)"
      ],
      "metadata": {
        "id": "training_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìà Step 10: Visualize Training"
      ],
      "metadata": {
        "id": "visualize"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training history\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "# Accuracy\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history['train_acc'], label='Train')\n",
        "plt.plot(history['val_acc'], label='Validation')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history['train_loss'], label='Train')\n",
        "plt.plot(history['val_loss'], label='Validation')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüìä Training Analysis:\")\n",
        "print(f\"   Final Train Acc: {history['train_acc'][-1]:.4f}\")\n",
        "print(f\"   Final Val Acc: {history['val_acc'][-1]:.4f}\")\n",
        "gap = history['train_acc'][-1] - history['val_acc'][-1]\n",
        "print(f\"   Accuracy Gap: {gap:.4f}\")\n",
        "if gap < 0.1:\n",
        "    print(\"   ‚úÖ Good! No overfitting detected\")\n",
        "elif gap < 0.2:\n",
        "    print(\"   ‚ö†Ô∏è Slight overfitting\")\n",
        "else:\n",
        "    print(\"   ‚ùå Overfitting detected!\")"
      ],
      "metadata": {
        "id": "visualize_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üíæ Step 11: Save Model"
      ],
      "metadata": {
        "id": "save"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model with metadata\n",
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'model_architecture': 'ResNet50',\n",
        "    'num_classes': num_classes,\n",
        "    'class_names': class_names,\n",
        "    'best_val_accuracy': history['val_acc'][-1],\n",
        "    'training_history': history,\n",
        "    'model_info': {\n",
        "        'architecture': 'ResNet50 with Transfer Learning',\n",
        "        'input_size': (224, 224),\n",
        "        'num_classes': num_classes,\n",
        "        'dropout_rate': 0.5,\n",
        "        'regularization': 'Dropout + BatchNorm + Data Augmentation',\n",
        "        'best_validation_accuracy': history['val_acc'][-1]\n",
        "    }\n",
        "}, 'skin_classifier.pth')\n",
        "\n",
        "# Save class mapping\n",
        "class_mapping = {str(i): name for i, name in enumerate(class_names)}\n",
        "with open('skin_classes.json', 'w') as f:\n",
        "    json.dump(class_mapping, f, indent=2)\n",
        "\n",
        "print(\"‚úÖ Model saved!\")\n",
        "print(\"   Files: skin_classifier.pth, skin_classes.json\")\n",
        "\n",
        "# Download files\n",
        "from google.colab import files\n",
        "files.download('skin_classifier.pth')\n",
        "files.download('skin_classes.json')\n",
        "\n",
        "print(\"\\nüì• Files downloaded! Copy them to your backend/models/ folder\")"
      ],
      "metadata": {
        "id": "save_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úÖ Training Complete!\n",
        "\n",
        "### Next Steps:\n",
        "\n",
        "1. **Download the model files** (done above)\n",
        "2. **Copy to your backend:**\n",
        "   ```bash\n",
        "   backend/models/skin_classifier.pth\n",
        "   backend/models/skin_classes.json\n",
        "   ```\n",
        "3. **Create API endpoint** for predictions\n",
        "4. **Test with real images**\n",
        "\n",
        "### Model Features:\n",
        "\n",
        "‚úÖ Transfer learning (ImageNet pre-trained)  \n",
        "‚úÖ Dropout (prevents overfitting)  \n",
        "‚úÖ Data augmentation (prevents overfitting)  \n",
        "‚úÖ Early stopping (prevents overfitting)  \n",
        "‚úÖ Learning rate scheduler (prevents underfitting)  \n",
        "‚úÖ Proper training duration  \n",
        "\n",
        "### Confidence Threshold:\n",
        "\n",
        "In your backend, set confidence threshold:\n",
        "- **‚â• 70%**: Show prediction\n",
        "- **< 70%**: Show \"Uncertain - consult a doctor\"\n",
        "\n",
        "This handles unknown diseases! üéØ"
      ],
      "metadata": {
        "id": "complete"
      }
    }
  ]
}
